#!/bin/bash

# Author: Christos Deligkaris
# Date: December 2022
# Purpose: This shell script submits a Microsim python job to the Ohio Supercomputer Center
# Note: This script is specific to Microsim because the python script is run using 
# 	the Microsim Poetry environment

# Slurm default arguments
defaultWalltime="--time=1:00:00"
defaultNtasks="--ntasks-per-node=1"
defaultNodes="--nodes=1"
defaultAccount="--account=PAS2139"

# check the arguments
if [ $# -lt 1 ]; then
	echo $'\nERROR: you need at least one argument (the input file)....\n'
	echo $'USAGE: submic yourPythonFile.py yourSlurmArguments\n'
	echo $"Slurm default arguments: $defaultWalltime $defaultNtasks $defaultNodes $defaultAccount"
	exit
fi

inputFile="$1"

# check if the input file is a python file
if [[ $inputFile != *.py ]] ; then
	echo $'\nERROR: input file is not a .py file\n'
	exit
fi

# check if the input file is an ordinary file and readable
if [ ! -f "$inputFile" ] || [ ! -r "$inputFile" ] ; then
        echo $"\nERROR: cannot find or read input file \"${inputFile}\"\n"
        exit
fi

outputFile="${1%.py}".log
stdoutputfile="stdoutput.log"
jobname=$(echo "$inputFile" | awk '{print substr($0,1,15)}')
#jobname=MIC-"${jobname}" #if you want to prepend this to the jobname

#set the microsim home directory so that this submission script can later copy microsim to the computing node
microsimDir="/fs/ess/PAS2139/christos/MICROSIM/CODE/microsim"

# pass the sbatch arguments
sbatch_arg=
for arg in $@; do
        if [ $arg != $0 ] && [ $arg != $1 ] ; then
                # pass the argument
                sbatch_arg="$sbatch_arg $arg"
	fi
done

# export the name of the input file to the environment so that SLURM can pass it on to python
#export INPUTFILE="$SLURM_SUBMIT_DIR"."$inputFile"

# submit the SLURM job
sbatch ${sbatch_arg} << EOF
#!/bin/bash
##################################################################

# note: slurm options specified on the command line will take precedence over slurm options in a job script

# note: Jobs are charged based length, number of cores, amount of memory, single node versus multi-node, and type of resource
# (so charges do not depend on which partition I use)
# https://www.osc.edu/supercomputing/knowledge-base/job_and_storage_charging

# Define the job name
#SBATCH --job-name=$jobname

# Set a pattern for the output file.
##SBATCH --output=$stdoutputfile
#SBATCH -e stderr.txt
#SBATCH -o stdout.txt
##SBATCH --error=\$TMPDIR/stderr.txt
##SBATCH --output=\$TMPDIR/stdout.txt

# all environment variables will be exported to the context of the job, including the name of the input file  
#SBATCH --export=ALL 

# default project account to be charged
#SBATCH --account=PAS2139

# number of CPUs
# An individual user can have up to 128 concurrently running jobs and/or up to 2040  processor cores in use on Pitzer. 
# All the users in a particular group/project can among them have up to 192 concurrently running jobs and/or up to 2040 processor 
# cores in use on Pitzer. Jobs submitted in excess of these limits are queued but blocked by the scheduler until other jobs 
# exit and free up resources.
#SBATCH --ntasks-per-node=1

# number of nodes
#SBATCH --nodes=1

# walltime 
# Serial jobs (that is, jobs which request only one node) can run for up to 168 hours (7 days), while parallel jobs may run for 
# up to 96 hours (4 days). Users who can demonstrate a need for longer serial job time may request access to the longserial queue, 
# which allows single-node jobs of up to 336 hours (14 days). 
# https://www.osc.edu/supercomputing/batch-processing-at-osc/scheduling-policies-and-limits
#SBATCH --time=1:00:00

# partition requested
# it is recommended to set ntasks and walltime and let SLURM choose partition, charges do not depend on which partition is used
##SBATCH --partition=

# sets when to send emails, options: BEGIN, END, FAIL, ALL
##SBATCH --mail-type=ALL 

##################################################################

# print date, time and store start time
date
startTime=\$SECONDS

# useful for debugging, prints each command in the log file as it is executed, with a + in front of it
#set -x

# path to a node-specific temporary directory (/tmp) for a given job, the name of this directory includes 
# the JOBID, so if two jobs run on the same node, each will definitely get their own TMPDIR
cd \$TMPDIR

# Copy input data to the nodes fast local disk
cp \$SLURM_SUBMIT_DIR/* ./

/fs/ess/PAS2139/christos/MICROSIM/CODE/VENV-CACHE/pypoetry/virtualenvs

# copy the poetry virtual environment to local disk and activate,
# need to export the poetry configuration variable for the cache directory in order for python to use the local python libraries
# and not the ones from the home directory where the virtual environment was created
mkdir -p VENV-CACHE/pypoetry/virtualenvs
cp -r /fs/ess/PAS2139/christos/MICROSIM/CODE/VENV-CACHE/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9 ./VENV-CACHE/pypoetry/virtualenvs/
export POETRY_CACHE_DIR=\$TMPDIR/VENV-CACHE/pypoetry/
source ./VENV-CACHE/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/bin/activate

# copy the entire microsim code to the local compute node
# otherwise the compute node will need to do a lot of IO operations to the home directory, 
# which strains home dir storage nodes
cp -r "$microsimDir" ./
cd microsim #poetry requires running python from the microsim directory

# if you are running an openMP job, this needs to be the same as ntasks (see above)
# if it is a serial job, no need to do anything
export OMP_NUM_THREADS=\$SLURM_NTASKS
#echo \$OMP_NUM_THREADS #test if export is working or not

# export the name of the input file to the environment so that SLURM can pass it on to python
# export INPUTFILE=\$SLURM_SUBMIT_DIR/"$inputFile"

# run the code, if output is not directed, it goes to standard output
#poetry run python \$TMPDIR/"$inputFile" > \$TMPDIR/"$outputFile"
#python \$TMPDIR/"$inputFile" > \$SLURM_SUBMIT_DIR/"$outputFile"
\$TMPDIR/VENV-CACHE/pypoetry/virtualenvs/microsim-gKvDNsM8-py3.9/bin/python3.9 \$TMPDIR/"$inputFile" > \$TMPDIR/"$outputFile"

# if your results go to the compute node, copy results to proper home folder
cd \$TMPDIR
cp results.csv \$SLURM_SUBMIT_DIR
cp "$outputFile" \$SLURM_SUBMIT_DIR

# print date, time and store end time
date
endTime=\$SECONDS

# print run time in hours and minutes (integer division)
echo "wall time  \$(((\$endTime - \$startTime)/3600)) hr or \$(((\$endTime - \$startTime)/60)) min"

EOF
